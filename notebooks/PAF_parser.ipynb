{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "\n",
    "INPUT_LENGTH = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULAR = \"reg\"\n",
    "CHIMERIC = \"chi\"\n",
    "REPEAT = \"rep\"\n",
    "LOW_QUALITY = \"loq\"\n",
    "\n",
    "\n",
    "class Sequence:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.query_id = \"-1\"\n",
    "        self.query_len = \"0\"\n",
    "        self.type = REGULAR\n",
    "        self.bps = np.zeros(0)\n",
    "\n",
    "    def setup(self, query_id, query_len):\n",
    "        self.query_id = query_id\n",
    "        self.query_len = query_len\n",
    "        self.bps = np.zeros(query_len)\n",
    "\n",
    "    def print(self, file_handle):\n",
    "        overlaps = \",\".join(map(str, self.bps.tolist()))\n",
    "        serialized = \"%s\\t%s\\t%s\\t%s\\n\" % (self.query_id, self.query_len, self.type, overlaps)\n",
    "        file_handle.write(serialized)\n",
    "\n",
    "    def append(self, query_hit_start, query_hit_end):\n",
    "        for index in range(query_hit_start, query_hit_end):\n",
    "            self.bps[index] += 1\n",
    "\n",
    "    def pad_sequence(self):\n",
    "        padding = INPUT_LENGTH - int(self.query_len)\n",
    "        if padding == 0:\n",
    "            return\n",
    "\n",
    "        extended_bps: List = self.bps.tolist()\n",
    "\n",
    "        i = 0\n",
    "        while padding > 0:\n",
    "            current_len = len(extended_bps)\n",
    "\n",
    "            bps_to_interpolate = min(padding, current_len)\n",
    "            interpolation_indices = random.sample(population=range(current_len), k=bps_to_interpolate)\n",
    "\n",
    "            last_index = current_len - 1\n",
    "            for idx in range(current_len):  # Go through existing BPs\n",
    "                if idx in interpolation_indices:\n",
    "                    if idx == last_index:\n",
    "                        point = (extended_bps[idx - 1] + extended_bps[idx]) // 2\n",
    "                        extended_bps.insert(idx, point)\n",
    "                    else:\n",
    "                        point = (extended_bps[idx] + extended_bps[idx + 1]) // 2\n",
    "                        extended_bps.insert(idx + 1, point)\n",
    "\n",
    "            padding = padding - bps_to_interpolate\n",
    "            i = i + 1\n",
    "\n",
    "        self.bps = np.array(extended_bps)\n",
    "\n",
    "    def trim_sequence(self):\n",
    "        excess = int(self.query_len) - INPUT_LENGTH\n",
    "        if excess == 0:\n",
    "            return\n",
    "\n",
    "        drop_indexes = random.sample(population=range(0, int(self.query_len)), k=excess)\n",
    "        self.bps = np.delete(self.bps, drop_indexes, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paf(input_file):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    line_count = file_line_count(input_file)\n",
    "\n",
    "    start = time.time()\n",
    "    with open(file=input_file, mode=\"r\", buffering=1) as paf_handle:\n",
    "        with open(file=\"../output/output_\" + timestamp + \".tsv\", mode=\"w+\") as out_handle:\n",
    "            out_handle.write(\"ID\\tLEN\\tCAT\\tPTS\\n\")\n",
    "            current_sequence = Sequence()\n",
    "\n",
    "            for idx, alignment in enumerate(paf_handle):\n",
    "                if idx % 1000 == 0:\n",
    "                    print(\"Parsing line {}. Percent complete = {:.4f}\\n\".format(idx, idx / line_count))\n",
    "\n",
    "                fields = alignment.split(sep=\"\\t\")\n",
    "\n",
    "                # Extract interesting fields\n",
    "                query_id = fields[0]\n",
    "                query_len = int(fields[1])\n",
    "                query_hit_start = int(fields[2])\n",
    "                query_hit_end = int(fields[3])\n",
    "\n",
    "                # Uninitialized sequence, init with ID & total length\n",
    "                if current_sequence.query_id == \"-1\":\n",
    "                    current_sequence.setup(query_id, query_len)\n",
    "\n",
    "                if current_sequence.query_id == query_id:\n",
    "                    # Still the same query sequence, increment existing values\n",
    "                    current_sequence.append(query_hit_start, query_hit_end)\n",
    "\n",
    "                    if idx == line_count - 1:\n",
    "                        save_sequence(current_sequence, out_handle)\n",
    "                        return\n",
    "                else:\n",
    "                    save_sequence(current_sequence, out_handle)\n",
    "\n",
    "                    # Setup new sequence & append information from current line\n",
    "                    current_sequence.setup(query_id, query_len)\n",
    "                    current_sequence.append(query_hit_start, query_hit_end)\n",
    "\n",
    "\n",
    "def save_sequence(sequence, out_handle):\n",
    "    if sequence.query_len < INPUT_LENGTH:\n",
    "        sequence.pad_sequence()\n",
    "    else:\n",
    "        sequence.trim_sequence()\n",
    "\n",
    "    # Write the current sequence to file\n",
    "    sequence.print(out_handle)\n",
    "\n",
    "\n",
    "def file_line_count(input_file):\n",
    "    out = subprocess.Popen(\n",
    "        ['wc', '-l', input_file],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT\n",
    "    ).communicate()[0]\n",
    "\n",
    "    return int(out.decode('utf-8').strip().split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count: 1463\n",
      "\n",
      "Parsing line 0. Percent complete = 0.0000\n",
      "\n",
      "Parsing line 1000. Percent complete = 0.6835\n",
      "\n",
      "Total execution time 1.71s\n"
     ]
    }
   ],
   "source": [
    "#Demo run (Set the path to your own PAF file as the file_path value)\n",
    "file_path = \"../data/sample.paf\"\n",
    "\n",
    "print(\"Line count: {}\\n\".format(file_line_count(file_path)))\n",
    "\n",
    "start = time.time()\n",
    "parse_paf(file_path)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total execution time {:.2f}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
