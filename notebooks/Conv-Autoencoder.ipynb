{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import Sequential, Input, Model, regularizers\n",
    "from tensorflow.python.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dropout, Dense, LSTM, Reshape, BatchNormalization, UpSampling1D\n",
    "from tensorflow.train import AdamOptimizer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Lambda, Conv2DTranspose\n",
    "\n",
    "\n",
    "REGULAR = \"reg\"\n",
    "CHIMERIC = \"chi\"\n",
    "REPEAT = \"rep\"\n",
    "LOW_QUALITY = \"loq\"\n",
    "\n",
    "INPUT_LENGTH = 5000\n",
    "INPUT_THRESHOLD = 1000\n",
    "\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convolutional_encoder(input_shape):\n",
    "    reshape = Reshape(target_shape=(INPUT_LENGTH, 1), input_shape=(INPUT_LENGTH,))(input_shape)\n",
    "    conv1 = Conv1D(filters=64, kernel_size=31, activation='relu', padding='same')(reshape)\n",
    "    conv1 = MaxPooling1D(pool_size=10)(conv1)\n",
    "    \n",
    "    conv2 = Conv1D(filters=32, kernel_size=31, activation='relu', padding='same')(conv1)\n",
    "    conv2 = MaxPooling1D(pool_size=5)(conv2)\n",
    "\n",
    "    print(\"shape of encoded {}\".format(K.int_shape(conv2)))\n",
    "    return conv2\n",
    "\n",
    "\n",
    "def create_convolutional_decoder(encoder):\n",
    "    conv3 = UpSampling1D(5)(encoder)\n",
    "    conv3 = Conv1D(filters=64, kernel_size=31, activation='relu', padding='same')(conv3)\n",
    "    \n",
    "    conv4 = UpSampling1D(10)(conv3)\n",
    "    conv4 = Conv1D(filters=64, kernel_size=31, activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    out = Conv1D(filters=1, kernel_size=31, activation='softmax', padding='same')(conv4)\n",
    "    out = Reshape((INPUT_LENGTH,))(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def create_conv_autoencoder():\n",
    "    input_shape = Input(shape=(INPUT_LENGTH, ))\n",
    "    model = Model(input_shape, create_convolutional_decoder(create_convolutional_encoder(input_shape)))\n",
    "\n",
    "    model.compile(optimizer=AdamOptimizer(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_datasets(tsv_input):\n",
    "    data = pd.read_csv(tsv_input, delimiter=\"\\t\")\n",
    "\n",
    "    # Filter out all low quality reads\n",
    "    data = data.loc[data.CAT != LOW_QUALITY]\n",
    "\n",
    "    # Convert sequence string to float array\n",
    "    data.PTS = data.PTS.apply(string_to_array)\n",
    "\n",
    "    # Convert labels from strings to ints\n",
    "    ys = data.CAT.apply(category_to_int).to_numpy()\n",
    "    xs = np.stack(data.PTS.array)\n",
    "\n",
    "    encoded_ys = to_categorical(ys, num_classes=NUM_CLASSES)\n",
    "\n",
    "    print(\"XS shape: {}\".format(xs.shape))\n",
    "    print(\"One-hot encoded YS shape: {}\".format(encoded_ys.shape))\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(xs, encoded_ys, test_size=0.15)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "def create_unlabeled_datasets(tsv_input):\n",
    "    data = pd.read_csv(tsv_input, delimiter=\"\\t\")\n",
    "\n",
    "    # Filter out all low quality reads\n",
    "    data = data.loc[data.CAT != LOW_QUALITY]\n",
    "\n",
    "    # Convert sequence string to float array\n",
    "    data.PTS = data.PTS.apply(string_to_array)\n",
    "    xs = np.stack(data.PTS.array)\n",
    "    print(\"Unlabeled XS shape: {}\".format(xs.shape))\n",
    "\n",
    "    train_x, test_x = train_test_split(xs, test_size=0.2)\n",
    "\n",
    "    return train_x, test_x\n",
    "\n",
    "\n",
    "def string_to_array(data):\n",
    "    data_string = str(data)\n",
    "    split = data_string.split(',')\n",
    "\n",
    "    return np.array([float(i) for i in split])\n",
    "\n",
    "\n",
    "def category_to_int(data):\n",
    "    category = str(data)\n",
    "    if category == REGULAR:\n",
    "        return 0\n",
    "    elif category == REPEAT:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "\n",
    "def evaluate_autoencoder(model, train_x, test_x, epochs_num):\n",
    "    history = model.fit(\n",
    "        x=train_x,\n",
    "        batch_size=64,\n",
    "        epochs=epochs_num,\n",
    "        validation_data=(test_x, None)\n",
    "    )\n",
    "\n",
    "    decoded_x = model.predict(test_x)\n",
    "    print(decoded_x.shape)\n",
    "    print(decoded_x[0].tolist())\n",
    "\n",
    "    plot_decoded(test_x, decoded_x)\n",
    "\n",
    "    plot_loss(history, epochs_num)\n",
    "\n",
    "\n",
    "def plot_decoded(original_x, decoded_x):\n",
    "    n = 20  # how many overlaps we will display\n",
    "    idxs = range(5000)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.plot(idxs, original_x[i].tolist())\n",
    "        plt.gray()\n",
    "        # display reconstruction\n",
    "        plt.subplot(2, n, i + 1 + n)\n",
    "        plt.plot(idxs, decoded_x[i].tolist())\n",
    "        plt.gray()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(model, epochs_num):\n",
    "    loss = model.history['loss']\n",
    "    val_loss = model.history['val_loss']\n",
    "    epochs = range(1, epochs_num + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(matrix):\n",
    "    df_cm = pd.DataFrame(matrix, [\"regular\", \"repeat\", \"chimeric\"], [\"regular\", \"repeat\", \"chimeric\"])\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sb.heatmap(df_cm, annot=True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def test_autoencoder(dataset):\n",
    "    model = create_conv_autoencoder()\n",
    "\n",
    "    print(\"Creating dataset...\")\n",
    "    train_x, test_x = create_unlabeled_datasets(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "    # Normalize inputs into [0, 1] range\n",
    "    train_x = train_x / np.max(train_x)\n",
    "    test_x = test_x / np.max(test_x)\n",
    "    \n",
    "    evaluate_autoencoder(model, train_x, test_x, epochs_num=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded (None, 100, 32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 5000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5000, 64)          2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 500, 32)           63520     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 500, 64)           63552     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1 (None, 5000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5000, 64)          127040    \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5000, 1)           1985      \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 5000)              0         \n",
      "=================================================================\n",
      "Total params: 258,145\n",
      "Trainable params: 258,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Creating dataset...\n",
      "Unlabeled XS shape: (7460, 5000)\n",
      "Dataset created!\n",
      "Train on 5968 samples, validate on 1492 samples\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cac75cd2e2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/floyd/input/overlaps/classified_7000.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0c6826f9bec5>\u001b[0m in \u001b[0;36mtest_autoencoder\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mevaluate_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0c6826f9bec5>\u001b[0m in \u001b[0;36mevaluate_autoencoder\u001b[0;34m(model, train_x, test_x, epochs_num)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0mindices_for_conversion_to_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m       \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_autoencoder(\"/floyd/input/overlaps/classified_7000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
